import pandas as pd
import matplotlib.pyplot as plt
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS
from time import time
from collections import defaultdict

# Parameters
strategy = None  # Set to None to analyze all strategies
continent = None  # Set to None to analyze all continents
year = [1990, 1994]  # Set to None to analyze all years
city = None
country = None
n_features = 1000
n_components = 5
n_top_words = 20

# File paths
data_file = '...\total_data_6.5.csv'
synonyms_file = '...\Expanded_Synonyms.csv'

# Load and preprocess data
def load_data(file_path, strategy=None, continent=None, year=None, city=None, country=None):
    df = pd.read_csv(file_path)
    print(f"Initial data count: {df.shape[0]}")

    # Filter data
    if year:
        df = df[df['Publication Year'].isin(year)]
        print(f"Data count after year filter: {df.shape[0]}")
    if strategy:
        df = df[df['Strategy'] == strategy]
        print(f"Data count after strategy filter: {df.shape[0]}")
    if continent:
        df = df[df['Continents'].str.contains(continent, na=False)]
        print(f"Data count after continent filter: {df.shape[0]}")
    if city:
        df = df[df['Cities'].str.contains(city, na=False)]
        print(f"Data count after city filter: {df.shape[0]}")
    if country:
        df = df[df['Countries'].isin(country)]
        print(f"Data count after country filter: {df.shape[0]}")

    data = df[['Title', 'Abstract', 'Keywords']].fillna('').apply(lambda x: ' '.join(x), axis=1)
    all_stop_words = list(ENGLISH_STOP_WORDS)

    return data, all_stop_words

# Preprocess data
def preprocess_data(data):
    return data.dropna()

# Load synonym mapping
def load_synonym_mapping(synonyms_file):
    synonym_df = pd.read_csv(synonyms_file)
    synonym_mapping = dict(zip(synonym_df['term'], synonym_df['replacement']))
    return synonym_mapping

# Merge similar terms
def merge_similar_terms(topics, synonym_mapping):
    merged_topics = []
    for topic in topics:
        merged_topic = defaultdict(float)
        for term, weight in topic:
            term = synonym_mapping.get(term, term)  # Replace term if it has a synonym in synonym_mapping
            merged_topic[term] += weight
        merged_topics.append(sorted(merged_topic.items(), key=lambda x: x[1], reverse=True))
    return merged_topics

# Plot top words
def plot_top_words(model, feature_names, n_top_words, title):
    fig, axes = plt.subplots(2, 3, figsize=(30, 15), sharex=True)
    axes = axes.flatten()
    topics = []
    for topic_idx, topic in enumerate(model.components_):
        top_features_ind = topic.argsort()[-n_top_words:]
        top_features = feature_names[top_features_ind]
        weights = topic[top_features_ind]
        topics.append(list(zip(top_features, weights)))

    synonym_mapping = load_synonym_mapping(synonyms_file)
    merged_topics = merge_similar_terms(topics, synonym_mapping)

    for topic_idx, topic in enumerate(merged_topics):
        top_features, weights = zip(*topic[:n_top_words])

        ax = axes[topic_idx]
        ax.barh(top_features, weights, height=0.7)
        ax.set_title(f"Topic {topic_idx + 1}", fontdict={"fontsize": 20})
        ax.tick_params(axis="both", which="major", labelsize=10)
        for spine in ["top", "right", "left"]:
            ax.spines[spine].set_visible(False)
        fig.suptitle(title, fontsize=30)

    plt.subplots_adjust(top=0.90, bottom=0.05, wspace=0.90, hspace=0.3)
    plt.show()

# Save plot
def save_plot(filename):
    plt.savefig(filename, bbox_inches='tight')

# Export top words
def export_top_words(model, feature_names, n_top_words, output_file):
    topics = []
    for topic_idx, topic in enumerate(model.components_):
        top_features_ind = topic.argsort()[:-n_top_words - 1:-1]
        top_features = feature_names[top_features_ind]
        weights = topic[top_features_ind]
        topics.append(list(zip(top_features, weights)))

    synonym_mapping = load_synonym_mapping(synonyms_file)
    merged_topics = merge_similar_terms(topics, synonym_mapping)

    export_data = []
    for topic_idx, topic in enumerate(merged_topics):
        for term, weight in topic[:n_top_words]:
            export_data.append({"Topic": topic_idx + 1, "Term": term, "Weight": weight})
    df_topics = pd.DataFrame(export_data)
    df_topics.to_csv(output_file, index=False)
    print(f"Top words and weights exported to {output_file}")

# Main program
def main():
    global filtered_data  # Declare global variable to store filtered data
    # Load and preprocess data
    print("Loading dataset...")
    t0 = time()
    data, all_stop_words = load_data(data_file, strategy, continent, year, city, country)
    data = preprocess_data(data)
    filtered_data = data  # Save filtered data to global variable
    print("done in %0.3fs." % (time() - t0))

    # Print filtered data
    print(f"Filtered data sample (first 5 rows):\n{data.head()}")
    print(f"Filtered data count: {data.shape[0]}")

    if data.empty:
        print("No data available after filtering.")
        return

    # Extract features
    print("Extracting tf features for LDA...")
    tf_vectorizer = CountVectorizer(
        max_df=0.95, min_df=1, max_features=n_features, stop_words=all_stop_words,
        ngram_range=(1, 3)
    )

    t0 = time()
    try:
        tf = tf_vectorizer.fit_transform(data)
    except ValueError as e:
        print(f"Error during vectorization: {e}")
        return
    print("done in %0.3fs." % (time() - t0))

    # Fit LDA model
    print("Fitting LDA models with tf features, n_features=%d..." % n_features)
    lda = LatentDirichletAllocation(
        n_components=n_components,
        max_iter=5,
        learning_method="online",
        learning_offset=50.0,
        random_state=0,
    )
    t0 = time()
    lda.fit(tf)
    print("done in %0.3fs." % (time() - t0))

    # Get feature names
    tf_feature_names = tf_vectorizer.get_feature_names_out()

    # Export results
    strategy_str = strategy if strategy else "all_strategies"
    continent_str = continent if continent else "all_continents"
    year_str = year if year else "_"
    city_str = city if city else "_"
    country_str = country if country else "_"
    plot_title = f"Topics in LDA model ({strategy_str}, {continent_str}, {year_str}, {city_str}, {country_str})"
    plot_top_words(lda, tf_feature_names, n_top_words, plot_title)

    plot_file = f"Figures_2.0/lda_top_words_{strategy_str}_{continent_str}_{year_str}_{city_str}_{country_str}.png"
    save_plot(plot_file)
    print(f"Plot saved to {plot_file}")

    output_file = f"Tables_2.0/lda_top_words_{strategy_str}_{continent_str}_{year_str}_{city_str}_{country_str}.csv"
    export_top_words(lda, tf_feature_names, n_top_words, output_file)

if __name__ == "__main__":
    main()
